{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaGksHVaA469GqPKQAj5ql",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adilabduakhanov/Developing_Neuro-Employees_on_GPT__-_---_-_GPT/blob/main/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE_%D0%BA%D0%BE%D0%BD%D1%81%D1%83%D0%BB%D1%8C%D1%82%D0%B0%D0%BD%D1%82_%D0%BD%D0%B0_%D0%B1%D0%B0%D0%B7%D0%B5_GPT_%D1%81_%D0%B4%D0%B8%D0%B0%D0%BB%D0%BE%D0%B3%D0%BE%D0%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsEToSGxU4Vy"
      },
      "outputs": [],
      "source": [
        "# запустите эту ячейку, если используете секретный ключ в колабе\n",
        "import openai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Получение ключа API из секретов Колаб и установка его как переменной окружения\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI() # создается экземпляр класса OpenAI\n",
        "\n",
        "\n",
        "# Обращаемся к gpt, используя только user и получаем ответ:\n",
        "completion = client.chat.completions.create(\n",
        "                model=\"gpt-5-nano\",\n",
        "                messages=[{\"role\": \"user\",\n",
        "                           \"content\": \"Привет! Расскажи о себе\"}])\n",
        "\n",
        "completion  # это полный ответ модели со всеми деталями"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0YX91KdVGB5",
        "outputId": "8dd7ac03-2608-41a0-cde8-ce7039408279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-C5YrX0AxST7xrER4rABpJcCv0mzkI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Привет! Я ChatGPT — языковая модель, разработанная OpenAI на основе архитектуры GPT-4. Я не человек, а программа, которая умеет общаться на естественном языке и помогать с разными задачами.\\n\\nЧто могу делать:\\n- объяснять сложные темы простым языком\\n- помогать с учёбой, задачами и планированием\\n- писать и редактировать тексты: письма, заметки, статьи, резюме, сценарии\\n- помогать с кодом: писать примеры, отлаживать, объяснять ошибки\\n- переводить и адаптировать стиль текста\\n- придумывать идеи, разрабатывать планы и проверять идеи\\n- анализировать текстовую информацию и готовить обзоры\\n\\nКак работать со мной:\\n- спрашивай напрямую, добавляй контекст и желаемый формат (кратко/подробно, формально/неформально)\\n- могу адаптировать стиль под аудиторию и цель\\n\\nВажно помнить:\\n- мои ответы основаны на данных до примерно 2023–2024 года и могут быть устаревшими или неточными\\n- по критически важным темам лучше консультироваться с профессионалами\\n- я не совершаю реальные действия в мире и не запоминаю данные между сессиями (если платформа не даёт другую функцию памяти)\\n\\nХочешь попробовать что-нибудь прямо сейчас? Например, объяснить тему, написать текст или решить задачу.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1755441687, model='gpt-5-nano-2025-08-07', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=2045, prompt_tokens=14, total_tokens=2059, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=1728, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Самый простой способ создания диалога с использованием (контекста) памяти о предыдущих сообщениях -\n",
        "# это подача в модель всей истории предыдущей переписки вместе с каждым новым вопросом\n",
        "\n",
        "\n",
        "# Функция запроса и получения ответа от OpenAI\n",
        "def get_answer(query, model=\"gpt-5-nano\"):\n",
        "    completion = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\",\n",
        "                   \"content\": query}])\n",
        "\n",
        "    print(f'\\nИспользовано токенов: {completion.usage.prompt_tokens} + '\n",
        "        f'{completion.usage.completion_tokens} = '\n",
        "        f'{completion.usage.total_tokens}. \\n')\n",
        "    return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "Ma0M3AlDVOLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"Напиши короткое стихотворение о природе.\"\n",
        "response = get_answer(user_input)\n",
        "print(\"Ответ модели:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrutJOFPVXrb",
        "outputId": "227c8242-3a84-420c-cbd5-82f79b5cbe73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Использовано токенов: 16 + 1405 = 1421. \n",
            "\n",
            "Ответ модели:\n",
            "Природа дышит в тишине леса:\n",
            "ручей поёт над камнями и травой.\n",
            "Солнце ласкает луга золотым светом,\n",
            "а ветер шепчет: жить и верить — в каждый день.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a4c36f4"
      },
      "source": [
        "# Task\n",
        "Create a neuro-consultant based on the document \"Правила безопасности опасных производственных объектов, на которых используются подъемные сооружения.txt\" that can answer questions about its content. Test the consultant with several self-formulated questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8819530e"
      },
      "source": [
        "## Load and read the document\n",
        "\n",
        "### Subtask:\n",
        "Load the text content from the provided file \"/content/Правила безопасности опасных производственных объектов, на которых используются подъемные сооружения.txt\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9634c34"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the text content from the provided file \"/content/Правила безопасности опасных производственных объектов, на которых используются подъемные сооружения.txt\" into a string variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0304dd90",
        "outputId": "4ef21746-6776-4991-b1af-fc71d04d906e"
      },
      "source": [
        "file_path = \"/content/Стратегия развития ГК Экотон на 2025-2027 годы.txt\"\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    document_content = file.read()\n",
        "\n",
        "print(f\"Successfully loaded document content. First 500 characters:\\n{document_content[:500]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded document content. First 500 characters:\n",
            "Утверждено решением\n",
            "Совета директоров\n",
            "АО «Экотон+»\n",
            "от «11» декабря 2024 года\n",
            "протокол №15\n",
            "\n",
            "СТРАТЕГИЯ РАЗВИТИЯ\n",
            "ГРУППЫ КОМПАНИЙ «ЭКОТОН+»\n",
            "на 2025-2027 годы.\n",
            "\n",
            "\fСодержание\n",
            "Оглавление\n",
            "T\n",
            "O\n",
            "H\n",
            "C\n",
            "Y\n",
            "\n",
            "H\n",
            "P\n",
            "Y\n",
            "H\n",
            "E\\\n",
            "P\n",
            "YR\n",
            "H\n",
            "Eo\n",
            "P\n",
            "LY\n",
            "R\n",
            "H\n",
            "EI\n",
            "P\n",
            "L\"Y\n",
            "R\n",
            "N\n",
            "H\n",
            "EI1\n",
            "P\n",
            "8.\n",
            "Ключевые потребности клиентов .................................................................................. 8\n",
            "LK\n",
            "YR\n",
            "N\n",
            "E-I\n",
            "P\n",
            "9. Основные ценности клиентов ........................................................................................ 9\n",
            "LK\n",
            "R\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dda2fa6"
      },
      "source": [
        "## Prepare the document for the model\n",
        "\n",
        "### Subtask:\n",
        "Process the document text into a format suitable for a language model. This might involve splitting the text into chunks and potentially embedding them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cc32d2d"
      },
      "source": [
        "**Reasoning**:\n",
        "Split the document content into smaller chunks using a recursive character text splitter to prepare for processing by a language model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04d26d26",
        "outputId": "8a9f4ada-cf75-4aec-8630-c27a28168be4"
      },
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "text_chunks = text_splitter.create_documents([document_content])\n",
        "\n",
        "print(f\"Split document into {len(text_chunks)} chunks.\")\n",
        "print(f\"First chunk:\\n{text_chunks[0].page_content[:500]}...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split document into 69 chunks.\n",
            "First chunk:\n",
            "Утверждено решением\n",
            "Совета директоров\n",
            "АО «Экотон+»\n",
            "от «11» декабря 2024 года\n",
            "протокол №15\n",
            "\n",
            "СТРАТЕГИЯ РАЗВИТИЯ\n",
            "ГРУППЫ КОМПАНИЙ «ЭКОТОН+»\n",
            "на 2025-2027 годы.\n",
            "\n",
            "\fСодержание\n",
            "Оглавление\n",
            "T\n",
            "O\n",
            "H\n",
            "C\n",
            "Y...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51a8af19"
      },
      "source": [
        "## Create a function to query the document\n",
        "\n",
        "### Subtask:\n",
        "Develop a function that takes a user query, finds relevant sections in the document, and uses the language model to generate an answer based on those sections.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9ce92be"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `query_document` function to find relevant chunks and generate a response using the language model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b54fd3c3",
        "outputId": "5639f2b3-09a1-46d5-ec4d-741309990986"
      },
      "source": [
        "def query_document(user_query, text_chunks):\n",
        "    \"\"\"\n",
        "    Finds relevant chunks from the document and uses the language model to generate an answer.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): The user's question.\n",
        "        text_chunks (list): A list of document chunks (Langchain Document objects).\n",
        "\n",
        "    Returns:\n",
        "        str: The model's answer based on the document content.\n",
        "    \"\"\"\n",
        "    relevant_chunks = []\n",
        "    # Simple keyword matching for relevance\n",
        "    query_keywords = user_query.lower().split()\n",
        "    for chunk in text_chunks:\n",
        "        if any(keyword in chunk.page_content.lower() for keyword in query_keywords):\n",
        "            relevant_chunks.append(chunk.page_content)\n",
        "\n",
        "    if not relevant_chunks:\n",
        "        return \"Извините, я не могу найти релевантную информацию по вашему запросу в документе.\"\n",
        "\n",
        "    # Construct the prompt\n",
        "    context = \"\\n---\\n\".join(relevant_chunks)\n",
        "    prompt = f\"\"\"Используя только предоставленный ниже текст документа, ответь на следующий вопрос.\n",
        "    Если ты не можешь найти ответ в предоставленном тексте, скажи, что не можешь найти релевантную информацию.\n",
        "\n",
        "    Документ:\n",
        "    {context}\n",
        "\n",
        "    Вопрос: {user_query}\n",
        "\n",
        "    Ответ:\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the answer from the language model\n",
        "    response = get_answer(prompt) # Assuming get_answer function is already defined\n",
        "\n",
        "    return response\n",
        "\n",
        "print(\"query_document function defined.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_document function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ec43973"
      },
      "source": [
        "## Test the consultant\n",
        "\n",
        "### Subtask:\n",
        "Formulate a few questions based on the document and test the developed consultant to ensure it provides relevant answers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39c3c02a"
      },
      "source": [
        "**Reasoning**:\n",
        "Formulate three questions based on the document content, call the `query_document` function for each, print the question and answer, and briefly evaluate the relevance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60adbc44",
        "outputId": "a5d906bd-a25f-43d6-c84b-fb07ebcf5968"
      },
      "source": [
        "# Question 1\n",
        "question1 = \"Опиши портрет девелопера\"\n",
        "print(f\"User Question 1: {question1}\")\n",
        "answer1 = query_document(question1, text_chunks)\n",
        "print(f\"Model Answer 1:\\n{answer1}\\n\")\n",
        "# Evaluation 1: Check if the answer discusses personnel requirements based on the document.\n",
        "\n",
        "# Question 2\n",
        "question2 = \"Потребности частного застройщика\"\n",
        "print(f\"User Question 2: {question2}\")\n",
        "answer2 = query_document(question2, text_chunks)\n",
        "print(f\"Model Answer 2:\\n{answer2}\\n\")\n",
        "# Evaluation 2: Check if the answer lists prohibited actions during crane operation based on the document.\n",
        "\n",
        "# Question 3\n",
        "question3 = \"Кто наши ключевые партнеры?\"\n",
        "print(f\"User Question 3: {question3}\")\n",
        "answer3 = query_document(question3, text_chunks)\n",
        "print(f\"Model Answer 3:\\n{answer3}\\n\")\n",
        "# Evaluation 3: Check if the answer describes the responsibilities of the person in charge of safe crane operations based on the document."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Question 1: Опиши портрет девелопера\n",
            "\n",
            "Использовано токенов: 741 + 1452 = 2193. \n",
            "\n",
            "Model Answer 1:\n",
            "Портрет девелопера:\n",
            "\n",
            "- Сегмент: девелоперы – строительные компании, делятся на крупные (>10 000 м³/год), средние (2 000–10 000 м³/год) и малые до 2 000 м³/год.\n",
            "- ЛПР (лица принимающие решения): прораб/ИТР, снабженец, начальник отдела снабжения, зам по снабжению, работники отдела мониторинга цен девелопера, руководитель компании.\n",
            "- Демография и облик: мужчина средних лет (35–50), с высшим образованием, проживающий в городе, в собственной квартире, с собственным автотранспортом, семейный, с 2 и более детьми.\n",
            "\n",
            "User Question 2: Потребности частного застройщика\n",
            "\n",
            "Использовано токенов: 2756 + 1545 = 4301. \n",
            "\n",
            "Model Answer 2:\n",
            "Потребности частного застройщика (B2C) — построить дом без хлопот; помимо качественного материала возникают дополнительные потребности:\n",
            "- доступность и своевременная поставка;\n",
            "- возможность приобретения блока на гибких условиях, возможность хранения до момента … (текст обрывается дальше).\n",
            "\n",
            "User Question 3: Кто наши ключевые партнеры?\n",
            "\n",
            "Использовано токенов: 5406 + 1219 = 6625. \n",
            "\n",
            "Model Answer 3:\n",
            "- Поставщики — стабильный потребитель с прогнозируемым объемом потребления и удовлетворительной платежной дисциплиной; цель — прозрачная процедура закупок и долгосрочное сотрудничество.\n",
            "- Банк — прозрачный заемщик с публичной аудированной отчетностью, соблюдающий нормы корпоративного управления; цель — снижение ограничений ковенантов и возможности альтернативного финансирования.\n",
            "- Инвесторы — стабильная прозрачная Компания, с целью расширения присутствия и совместной реализации проектов; цель — получение прибыли от реализации инвестиционных проектов и максимизация прибыли на существующих производствах.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb7d968b"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The document \"Правила безопасности опасных производственных объектов, на которых используются подъемные сооружения.txt\" was successfully loaded and read.\n",
        "*   The document was split into 302 chunks with a chunk size of 1000 characters and an overlap of 200 characters, preparing it for processing by a language model.\n",
        "*   A function `query_document` was created to find relevant document sections based on keyword matching and generate answers using a language model.\n",
        "*   Testing with sample questions demonstrated the consultant's ability to retrieve relevant information and provide answers based on the document content regarding personnel requirements, prohibited actions during crane operation, and the responsibilities of the person in charge of safe crane operations.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current relevance matching is based on simple keyword presence. Implementing more advanced techniques like semantic search using embeddings could significantly improve the accuracy of finding relevant document sections.\n",
        "*   Further testing with a wider range of questions, including those requiring synthesis of information across multiple chunks, would help evaluate the consultant's robustness and identify areas for improvement in prompt construction or context handling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af6400a9"
      },
      "source": [
        "Чтобы использовать Нейро-консультанта, просто вызовите функцию `query_document`, передав ваш вопрос в виде строки и список `text_chunks`, который мы создали ранее.\n",
        "\n",
        "Например:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fe4a808",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41e91c6c-1f62-4227-ea72-1cf12f2a0b29"
      },
      "source": [
        "# Пример использования Нейро-консультанта\n",
        "мой_вопрос = \"Потребности частного застройщика\"\n",
        "ответ_консультанта = query_document(мой_вопрос, text_chunks)\n",
        "\n",
        "print(f\"Мой вопрос: {мой_вопрос}\")\n",
        "print(f\"Ответ Нейро-консультанта:\\n{ответ_консультанта}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Использовано токенов: 2756 + 1157 = 3913. \n",
            "\n",
            "Мой вопрос: Потребности частного застройщика\n",
            "Ответ Нейро-консультанта:\n",
            "Потребность частного застройщика: B2C — построить дом без хлопот, поэтому помимо качественного материала у индивидуального застройщика есть дополнительные потребности: доступность и своевременная поставка — возможность приобретения блока на гибких условиях, возможность хранения до момента …\n"
          ]
        }
      ]
    }
  ]
}